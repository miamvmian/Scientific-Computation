{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVi_jp9DOpL"
      },
      "source": [
        "# MPI (Message-Passing Interface)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "* Mechanism of data exchange between processes\n",
        "* Two types of communication:\n",
        " * **point-to-point**: between two processes\n",
        " * **collective**: between multiple processes\n",
        "* Typically only one program, branching depending on the process\n",
        "* Using the mpi4py Python library\n",
        "\n",
        "An mpi4py tutorial:\n",
        "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
        "\n",
        "\n",
        "Install mpi4py:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgyeyISlEBPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e35c32-e808-4e10-ec25-6486fa1f3db0"
      },
      "source": [
        "!pip install mpi4py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.0.tar.gz (464 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/464.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m460.8/464.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4266268 sha256=d6fdc46335949d615bca63b0ee0f6ded2b4020b8ecacbce66990880f74989d24\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pnn-HgDpjV"
      },
      "source": [
        "## A basic example (no data exchange)\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk1x92wcDOpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e702180b-bba6-40c5-dcc2-b0c30bfe4651"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank() # index of the current process\n",
        "print (\"hello from process \", rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 4 python mpi.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRqPHFyAv-4I",
        "outputId": "b62b22a3-4dfd-45bc-d94c-0484ddca40b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  3\n",
            "hello from process  2\n",
            "hello from process  0\n",
            "hello from process  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptBtCQoDOqK"
      },
      "source": [
        "## Point-to-point communication of two processes\n",
        "\n",
        "### Example: computing $\\pi$ with MPI\n",
        "\n",
        "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
        "\n",
        "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
        "\n",
        "Without MPI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeSQbxbDOqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165cd9d2-f379-45b4-a7c6-af149700ff9e"
      },
      "source": [
        "import numpy as np\n",
        "a = np.arange(1,200000)\n",
        "print (np.sqrt(6*np.sum(1./(a*a))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1415878789259364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFHlcHIDOqz"
      },
      "source": [
        "### Functions ``send()``, ``recv()``\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NFhjMInDOq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e13ba1e-6ef2-404f-c4fc-91a5fb0e5057"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
        "\n",
        "# process 1 sends its partial sum to process 0\n",
        "if rank == 1:\n",
        "    comm.send(s, dest=0)\n",
        "\n",
        "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
        "# and outputs the result\n",
        "elif rank == 0:\n",
        "    s_other = comm.recv(source=1)\n",
        "    s_total = s+s_other\n",
        "    print ('total partial sum =', s_total)\n",
        "    print ('pi_approx =', np.sqrt(6*s_total))\n",
        "\n",
        "print ('Process', rank, 'finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaN4fOCFnfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664911cd-4ee8-4834-a86b-c4f94e0578a3"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 found partial sum from term 1 to term 100 :  1.6349839001848931\n",
            "Process 1 found partial sum from term 101 to term 200 :  0.004962645830104402\n",
            "Process 1 finished\n",
            "total partial sum = 1.6399465460149976\n",
            "pi_approx = 3.1368263063309683\n",
            "Process 0 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wSXWdXH1pY"
      },
      "source": [
        "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
        "\n",
        "## Collective communication\n",
        "\n",
        "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes.\n",
        "\n",
        "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5OC2CNDOrY"
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(filename=\"fastSum.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5eg3eZdDOr1"
      },
      "source": [
        "### Function ``gather()``\n",
        "\n",
        "Pass data from all processes to the chosen process.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNWAOcPDOr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f74afe6-27e2-445b-9834-ad91fe596bb7"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # total number of processes\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "\n",
        "partialSums = comm.gather(s, root=0)\n",
        "print ('partialSums gathered at process %d:' %(rank), partialSums)\n",
        "\n",
        "if rank == 0:\n",
        "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUfMyhSGG6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43d18b2-65b4-4d27-b4e8-37f07e842c49"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partialSums gathered at process 3: None\n",
            "partialSums gathered at process 2: None\n",
            "partialSums gathered at process 1: None\n",
            "partialSums gathered at process 4: None\n",
            "partialSums gathered at process 0: [1.6349839001848931, 0.004962645830104402, 0.0016597368826256017, 0.0008309063464401552, 0.0004988762708311448]\n",
            "pi_approx = 3.1396841231387222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXH5iJXICPd"
      },
      "source": [
        "### Function ``bcast()``\n",
        "\n",
        "Pass data from the chosen process to all other processes.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw2GT71eDOsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104aca05-a0bf-4d29-afd7-5dece26508df"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "# basic usage of bcast()\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    some_data = {0: 'abcd', 1:1234}\n",
        "else:\n",
        "    some_data = None\n",
        "\n",
        "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
        "data = comm.bcast(some_data, root=0)\n",
        "print (\"I'm process\", rank, '; data after broadcasting:', data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRlOFLVFGL-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae46533-662a-4e07-82bf-95622b27de53"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 3 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm process 0 ; data before broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 0 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data before broadcasting: None\n",
            "I'm process 1 ; data before broadcasting: None\n",
            "I'm process 1 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data after broadcasting: {0: 'abcd', 1: 1234}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm6buMYITTW"
      },
      "source": [
        "### Functions ``scatter()``, ``reduce()``\n",
        "\n",
        "* ``scatter()``: distribute data from one source to all processes\n",
        "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTGFw1YLDOsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7930f6f1-f77b-4ae6-f985-b4dc72a0da4b"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "# Evaluate the sum of N terms by scattering them to N processes.\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    data2scatter = [a*a for a in range(1,size+1)]\n",
        "else:\n",
        "    data2scatter = None\n",
        "\n",
        "data = comm.scatter(data2scatter, root=0)\n",
        "\n",
        "print ('Data at process', rank, ':', data)\n",
        "\n",
        "b = 1./data\n",
        "\n",
        "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
        "\n",
        "print ('Partial sum at process', rank, ':', partialSum)\n",
        "\n",
        "if rank == 0:\n",
        "    result = np.sqrt(6*partialSum)\n",
        "    print ('Pi_approx:', result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5NWqw3HGRTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9ebed6-03c0-42ae-9e73-b131491f24cc"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data at process 0 : 1\n",
            "Data at process 2 : 9\n",
            "Data at process 3 : 16\n",
            "Data at process 1 : 4\n",
            "Data at process 4 : 25\n",
            "Partial sum at process 3 : None\n",
            "Partial sum at process 4 : None\n",
            "Partial sum at process 2 : None\n",
            "Partial sum at process 1 : None\n",
            "Partial sum at process 0 : 1.4636111111111112\n",
            "Pi_approx: 2.9633877010385707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tmm87fhIdwU"
      },
      "source": [
        "## Example: parallel scalar product\n",
        "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$\n",
        "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
        "* Compute scalar products between chunks at each process\n",
        "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiixH1tCDOs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3479a7-5c2f-4878-b155-91064a8aea0e"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #length of vectors\n",
        "\n",
        "#arbitrary example vectors, generated to be evenly divided by the number of\n",
        "#processes for convenience\n",
        "\n",
        "x = np.random.rand(N) if comm.rank == 0 else None\n",
        "y = np.random.rand(N) if comm.rank == 0 else None\n",
        "\n",
        "#initialize as numpy arrays\n",
        "dot = np.array([0.])\n",
        "local_N = np.array([0])\n",
        "\n",
        "#test for conformability\n",
        "if rank == 0:\n",
        "                if (N != y.size):\n",
        "                                print (\"vector length mismatch\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #currently, our program cannot handle sizes that are not evenly divided by\n",
        "                #the number of processors\n",
        "                if (N % size != 0):\n",
        "                                print (\"the number of processors must evenly divide n.\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #length of each process's portion of the original vector\n",
        "                local_N = np.array([N//size])\n",
        "\n",
        "#communicate local array size to all processes\n",
        "comm.Bcast(local_N, root=0)\n",
        "\n",
        "#initialize as numpy arrays\n",
        "local_x = np.zeros(local_N)\n",
        "local_y = np.zeros(local_N)\n",
        "\n",
        "#divide up vectors\n",
        "comm.Scatter(x, local_x, root=0)\n",
        "comm.Scatter(y, local_y, root=0)\n",
        "\n",
        "#local computation of dot product\n",
        "local_dot = np.array([np.dot(local_x, local_y)])\n",
        "\n",
        "#sum the results of each\n",
        "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "                print (\"The dot product computed with MPI:\", dot[0])\n",
        "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjotqyGTHGfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d354e32-85e8-4d54-ddb7-e2f04f8812f1"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 10 python mpi.py 4000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product computed with MPI: 1001023.8592246629\n",
            "The dot product computed w/o  MPI: 1001023.8592246614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcjQGyquDOtQ"
      },
      "source": [
        "\n",
        "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
        "\n",
        "### Reduce-based computation of $\\pi$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODozaK9rDOtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c78a0a-4423-4937-b6bc-a19a3502ae24"
      },
      "source": [
        "%%writefile mpi.py\n",
        "\n",
        "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #number of terms\n",
        "\n",
        "#initialize as numpy array\n",
        "s = np.array([0.])\n",
        "\n",
        "#test for conformability\n",
        "if (N % size != 0):\n",
        "    print (\"the number of processors must evenly divide n.\")\n",
        "    comm.Abort()\n",
        "\n",
        "#length of each process's portion of the original vector\n",
        "local_N = np.array([N/size])\n",
        "\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "#local computation of partial sum\n",
        "local_s = getPartialSum(1+rank*local_N[0], 1+(rank+1)*local_N[0])\n",
        "local_s = np.array([local_s])\n",
        "\n",
        "#sum the results of each local sum\n",
        "comm.Reduce(local_s, s, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "    pi_approx = np.sqrt(6*s[0])\n",
        "    print (\"pi_approx:\", pi_approx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOuSPxKJDOtl"
      },
      "source": [
        "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
        "\n",
        "(`real`: wall clock time).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIc3lRQwJFjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be6256c-3987-4549-ddc1-0f4d3efedca5"
      },
      "source": [
        "!time mpirun --allow-run-as-root --oversubscribe -n 1 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 4 python mpi.py 100000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pi_approx: 3.14159264404049\n",
            "\n",
            "real\t0m1.867s\n",
            "user\t0m0.717s\n",
            "sys\t0m1.018s\n",
            "pi_approx: 3.1415926440404958\n",
            "\n",
            "real\t0m1.603s\n",
            "user\t0m1.454s\n",
            "sys\t0m1.036s\n",
            "pi_approx: 3.1415926440404975\n",
            "\n",
            "real\t0m2.540s\n",
            "user\t0m2.451s\n",
            "sys\t0m1.856s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLSL8ssHKvq3"
      },
      "source": [
        "Speedup and efficiency of parallelization with 2 processes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMya2FYDOto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103c9fd7-f1b7-4d28-b2d5-f0a0f6bf7597"
      },
      "source": [
        "Speedup = 3.084/1.652\n",
        "print ('Speedup:', Speedup)\n",
        "\n",
        "Efficiency = Speedup/2\n",
        "print ('Efficiency:', Efficiency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup: 1.8668280871670704\n",
            "Efficiency: 0.9334140435835352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()\n",
        "print ('cores:', cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rU4zdO8oHK8",
        "outputId": "9763c17c-1032-4dc4-9962-e4dc390a5f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cores: 2\n"
          ]
        }
      ]
    }
  ]
}