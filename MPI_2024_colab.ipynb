{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVi_jp9DOpL"
      },
      "source": [
        "# MPI (Message-Passing Interface)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "* Mechanism of data exchange between processes\n",
        "* Two types of communication:\n",
        " * **point-to-point**: between two processes\n",
        " * **collective**: between multiple processes\n",
        "* Typically only one program, branching depending on the process\n",
        "* Using the mpi4py Python library\n",
        "\n",
        "An mpi4py tutorial:\n",
        "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
        "\n",
        "\n",
        "Install mpi4py:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgyeyISlEBPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1f4b2c-f034-428b-d3f7-59a3736462e1"
      },
      "source": [
        "!pip install mpi4py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.0.tar.gz (464 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/464.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m460.8/464.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4266263 sha256=e6ee7b24c981325b7f576391dd0ea261674a25475d5fa5c5d29da79b60b755c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pnn-HgDpjV"
      },
      "source": [
        "## A basic example (no data exchange)\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk1x92wcDOpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95034dd3-70d7-4537-fa45-3badd2d01c1e"
      },
      "source": [
        "# mpi.py\n",
        "from mpi4py import MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank() # index of the current process\n",
        "print (\"hello from process \", rank)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 4 python mpi.py"
      ],
      "metadata": {
        "id": "DRqPHFyAv-4I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptBtCQoDOqK"
      },
      "source": [
        "## Point-to-point communication of two processes\n",
        "\n",
        "### Example: computing $\\pi$ with MPI\n",
        "\n",
        "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
        "\n",
        "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
        "\n",
        "Without MPI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeSQbxbDOqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475dbb59-805a-431b-e4e2-42ee49036b94"
      },
      "source": [
        "import numpy as np\n",
        "a = np.arange(1,200000)\n",
        "print (np.sqrt(6*np.sum(1./(a*a))))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1415878789259364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFHlcHIDOqz"
      },
      "source": [
        "### Functions ``send()``, ``recv()``\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NFhjMInDOq4"
      },
      "source": [
        "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
        "\n",
        "# process 1 sends its partial sum to process 0\n",
        "if rank == 1:\n",
        "    comm.send(s, dest=0)\n",
        "\n",
        "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
        "# and outputs the result\n",
        "elif rank == 0:\n",
        "    s_other = comm.recv(source=1)\n",
        "    s_total = s+s_other\n",
        "    print ('total partial sum =', s_total)\n",
        "    print ('pi_approx =', np.sqrt(6*s_total))\n",
        "\n",
        "print ('Process', rank, 'finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaN4fOCFnfN"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wSXWdXH1pY"
      },
      "source": [
        "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
        "\n",
        "## Collective communication\n",
        "\n",
        "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes.\n",
        "\n",
        "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5OC2CNDOrY"
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(filename=\"fastSum.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5eg3eZdDOr1"
      },
      "source": [
        "### Function ``gather()``\n",
        "\n",
        "Pass data from all processes to the chosen process.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNWAOcPDOr-"
      },
      "source": [
        "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # total number of processes\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "\n",
        "partialSums = comm.gather(s, root=0)\n",
        "print ('partialSums gathered at process %d:' %(rank), partialSums)\n",
        "\n",
        "if rank == 0:\n",
        "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUfMyhSGG6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a4bf05-7380-4ca3-dadd-e6c0814f5570"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partialSums gathered at process 3: None\n",
            "partialSums gathered at process 2: None\n",
            "partialSums gathered at process 1: None\n",
            "partialSums gathered at process 4: None\n",
            "partialSums gathered at process 0: [1.6349839001848931, 0.004962645830104402, 0.0016597368826256017, 0.0008309063464401552, 0.0004988762708311448]\n",
            "pi_approx = 3.1396841231387222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXH5iJXICPd"
      },
      "source": [
        "### Function ``bcast()``\n",
        "\n",
        "Pass data from the chosen process to all other processes.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw2GT71eDOsY"
      },
      "source": [
        "# basic usage of bcast()\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    some_data = {0: 'abcd', 1:1234}\n",
        "else:\n",
        "    some_data = None\n",
        "\n",
        "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
        "data = comm.bcast(some_data, root=0)\n",
        "print (\"I'm process\", rank, '; data after broadcasting:', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRlOFLVFGL-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bbba418-83a6-4e3f-c301-769625377953"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 3 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm process 1 ; data before broadcasting: None\n",
            "I'm process 0 ; data before broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data before broadcasting: None\n",
            "I'm process 0 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 1 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data after broadcasting: {0: 'abcd', 1: 1234}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm6buMYITTW"
      },
      "source": [
        "### Functions ``scatter()``, ``reduce()``\n",
        "\n",
        "* ``scatter()``: distribute data from one source to all processes\n",
        "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTGFw1YLDOsq"
      },
      "source": [
        "# Evaluate the sum of N terms by scattering them to N processes.\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    data2scatter = [a*a for a in range(1,size+1)]\n",
        "else:\n",
        "    data2scatter = None\n",
        "\n",
        "data = comm.scatter(data2scatter, root=0)\n",
        "\n",
        "print ('Data at process', rank, ':', data)\n",
        "\n",
        "b = 1./data\n",
        "\n",
        "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
        "\n",
        "print ('Partial sum at process', rank, ':', partialSum)\n",
        "\n",
        "if rank == 0:\n",
        "    result = np.sqrt(6*partialSum)\n",
        "    print ('Pi_approx:', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5NWqw3HGRTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40881056-c4f5-44d3-fbcb-1813e1c1ce07"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python mpi.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data at process 0 : 1\n",
            "Data at process 2 : 9\n",
            "Data at process 4 : 25\n",
            "Data at process 3 : 16\n",
            "Data at process 1 : 4\n",
            "Partial sum at process 4 : None\n",
            "Partial sum at process 3 : None\n",
            "Partial sum at process 2 : None\n",
            "Partial sum at process 1 : None\n",
            "Partial sum at process 0 : 1.4636111111111112\n",
            "Pi_approx: 2.9633877010385707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tmm87fhIdwU"
      },
      "source": [
        "## Example: parallel scalar product\n",
        "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$\n",
        "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
        "* Compute scalar products between chunks at each process\n",
        "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiixH1tCDOs9"
      },
      "source": [
        "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #length of vectors\n",
        "\n",
        "#arbitrary example vectors, generated to be evenly divided by the number of\n",
        "#processes for convenience\n",
        "\n",
        "x = np.random.rand(N) if comm.rank == 0 else None\n",
        "y = np.random.rand(N) if comm.rank == 0 else None\n",
        "\n",
        "#initialize as numpy arrays\n",
        "dot = np.array([0.])\n",
        "local_N = np.array([0])\n",
        "\n",
        "#test for conformability\n",
        "if rank == 0:\n",
        "                if (N != y.size):\n",
        "                                print (\"vector length mismatch\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #currently, our program cannot handle sizes that are not evenly divided by\n",
        "                #the number of processors\n",
        "                if (N % size != 0):\n",
        "                                print (\"the number of processors must evenly divide n.\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #length of each process's portion of the original vector\n",
        "                local_N = np.array([N//size])\n",
        "\n",
        "#communicate local array size to all processes\n",
        "comm.Bcast(local_N, root=0)\n",
        "\n",
        "#initialize as numpy arrays\n",
        "local_x = np.zeros(local_N)\n",
        "local_y = np.zeros(local_N)\n",
        "\n",
        "#divide up vectors\n",
        "comm.Scatter(x, local_x, root=0)\n",
        "comm.Scatter(y, local_y, root=0)\n",
        "\n",
        "#local computation of dot product\n",
        "local_dot = np.array([np.dot(local_x, local_y)])\n",
        "\n",
        "#sum the results of each\n",
        "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "                print (\"The dot product computed with MPI:\", dot[0])\n",
        "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjotqyGTHGfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee6e691-ea70-4cf0-e9b7-c958fdf7e236"
      },
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 10 python mpi.py 8000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product computed with MPI: 2000304.7972726244\n",
            "The dot product computed w/o  MPI: 2000304.7972726333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LwwahO9zOV9t",
        "outputId": "d31bc5d5-f6cc-4395-f6a5-12eff7908dca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcjQGyquDOtQ"
      },
      "source": [
        "\n",
        "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
        "\n",
        "Expectation of the multiplication of x and y:\n",
        "$E(XY) = ∑E(x,y)=N*E(x_iy_j)$\n",
        "\n",
        "### Reduce-based computation of $\\pi$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODozaK9rDOtS"
      },
      "source": [
        "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #number of terms\n",
        "\n",
        "#initialize as numpy array\n",
        "s = np.array([0.])\n",
        "\n",
        "#test for conformability\n",
        "if (N % size != 0):\n",
        "    print (\"the number of processors must evenly divide n.\")\n",
        "    comm.Abort()\n",
        "\n",
        "#length of each process's portion of the original vector\n",
        "local_N = np.array([N/size])\n",
        "\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "#local computation of partial sum\n",
        "local_s = getPartialSum(1+rank*local_N[0], 1+(rank+1)*local_N[0])\n",
        "local_s = np.array([local_s])\n",
        "\n",
        "#sum the results of each local sum\n",
        "comm.Reduce(local_s, s, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "    pi_approx = np.sqrt(6*s[0])\n",
        "    print (\"pi_approx:\", pi_approx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOuSPxKJDOtl"
      },
      "source": [
        "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
        "\n",
        "(`real`: wall clock time).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIc3lRQwJFjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f22e28-4f39-4b3b-d238-ea653bcb6727"
      },
      "source": [
        "!time mpirun --allow-run-as-root --oversubscribe -n 1 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 2 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root --oversubscribe -n 4 python mpi.py 100000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pi_approx: 3.14159264404049\n",
            "\n",
            "real\t0m2.621s\n",
            "user\t0m0.634s\n",
            "sys\t0m1.834s\n",
            "pi_approx: 3.1415926440404958\n",
            "\n",
            "real\t0m1.438s\n",
            "user\t0m1.303s\n",
            "sys\t0m0.933s\n",
            "pi_approx: 3.1415926440404975\n",
            "\n",
            "real\t0m1.859s\n",
            "user\t0m1.888s\n",
            "sys\t0m1.206s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLSL8ssHKvq3"
      },
      "source": [
        "\\Speedup and efficiency of parallelization with n=2 processes:\n",
        "if the computation divided into non-parallel part $s$ and parellel part $s-1$.\n",
        "\n",
        "The speed up: $1/(s+(1-s)/n)$\n",
        "Efficiency: $1/(sn+(1-s))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMya2FYDOto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61eb2d7a-b3d7-4987-a4e7-a110c0c992bd"
      },
      "source": [
        "Speedup = 2.62/1.43\n",
        "print ('Speedup:', Speedup)\n",
        "# 2 is the number of tasks\n",
        "Efficiency = Speedup/2\n",
        "print ('Efficiency:', Efficiency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup: 1.8321678321678323\n",
            "Efficiency: 0.9160839160839161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()\n",
        "print ('cores:', cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rU4zdO8oHK8",
        "outputId": "3aa3a88a-52e8-479f-d83b-e0279b6abced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cores: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jws5znyXSTm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}